CONFIG
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                                                                                                                                                                     
│       devices: auto                                                                                                                                                                                                           
│       accelerator: cpu                                                                                                                                                                                                        
│       accumulate_grad_batches: 1                                                                                                                                                                                              
│       max_epochs: 1000                                                                                                                                                                                                        
│       min_epochs: 10                                                                                                                                                                                                          
│       max_steps: -1                                                                                                                                                                                                           
│       check_val_every_n_epoch: 1                                                                                                                                                                                              
│       fast_dev_run: false                                                                                                                                                                                                     
│                                                                                                                                                                                                                               
├── model
│   └── discretizer:                                                                                                                                                                                                            
│         entmax:                                                                                                                                                                                                               
│           _target_: src.models.modules.discrete_layers.entmax.EntmaxDiscreteLayer                                                                                                                                             
│           alpha: 1.1                                                                                                                                                                                                          
│         gumbel:                                                                                                                                                                                                               
│           _target_: src.models.modules.discrete_layers.gumbel.Gumbel                                                                                                                                                          
│           params: null                                                                                                                                                                                                        
│       collator:                                                                                                                                                                                                               
│         tokenizer:                                                                                                                                                                                                            
│           _target_: src.models.collators.tokenizers.simple_word_level_tokenizer.SimpleWordLevelTokenizer                                                                                                                      
│           model_max_length: 100                                                                                                                                                                                               
│           vocab_size: 30                                                                                                                                                                                                      
│           special_tokens:                                                                                                                                                                                                     
│           - '[pad]'                                                                                                                                                                                                           
│           - '[bos]'                                                                                                                                                                                                           
│           - '[eos]'                                                                                                                                                                                                           
│           - '[unk]'                                                                                                                                                                                                           
│           pad_token_id: 0                                                                                                                                                                                                     
│           batch_size: 4                                                                                                                                                                                                       
│         _target_: src.models.collators.simple_text_collator.SimpleTextCollator                                                                                                                                                
│         max_length: 100                                                                                                                                                                                                       
│         padding_side: right                                                                                                                                                                                                   
│         special_tokens:                                                                                                                                                                                                       
│         - '[pad]'                                                                                                                                                                                                             
│         - '[bos]'                                                                                                                                                                                                             
│         - '[eos]'                                                                                                                                                                                                             
│         - '[unk]'                                                                                                                                                                                                             
│         pad_token_id: 0                                                                                                                                                                                                       
│         max_X_length: 100                                                                                                                                                                                                     
│         max_Z_length: 100                                                                                                                                                                                                     
│         padding: true                                                                                                                                                                                                         
│       lr_scheduler:                                                                                                                                                                                                           
│         _target_: torch.optim.lr_scheduler.ReduceLROnPlateau                                                                                                                                                                  
│         mode: min                                                                                                                                                                                                             
│         factor: 0.8                                                                                                                                                                                                           
│         patience: 1                                                                                                                                                                                                           
│         threshold: 0.01                                                                                                                                                                                                       
│         threshold_mode: abs                                                                                                                                                                                                   
│         cooldown: 1                                                                                                                                                                                                           
│         min_lr: 1.0e-06                                                                                                                                                                                                       
│         eps: 1.0e-08                                                                                                                                                                                                          
│         verbose: true                                                                                                                                                                                                         
│         interval: epoch                                                                                                                                                                                                       
│         frequency: 1                                                                                                                                                                                                          
│         monitor: train/loss                                                                                                                                                                                                   
│         model_x_to_z_scheduler:                                                                                                                                                                                               
│           _target_: torch.optim.lr_scheduler.ReduceLROnPlateau                                                                                                                                                                
│           mode: min                                                                                                                                                                                                           
│           factor: 0.8                                                                                                                                                                                                         
│           patience: 1                                                                                                                                                                                                         
│           threshold: 0.01                                                                                                                                                                                                     
│           threshold_mode: abs                                                                                                                                                                                                 
│           cooldown: 1                                                                                                                                                                                                         
│           min_lr: 1.0e-06                                                                                                                                                                                                     
│           eps: 1.0e-08                                                                                                                                                                                                        
│           verbose: true                                                                                                                                                                                                       
│         model_x_to_z_scheduler_dict:                                                                                                                                                                                          
│           interval: epoch                                                                                                                                                                                                     
│           frequency: 1                                                                                                                                                                                                        
│           monitor: train/loss                                                                                                                                                                                                 
│           name: LearningRateScheduler_torch.optim.lr_scheduler.ReduceLROnPlateau                                                                                                                                              
│         model_z_to_x_scheduler:                                                                                                                                                                                               
│           _target_: torch.optim.lr_scheduler.ReduceLROnPlateau                                                                                                                                                                
│           mode: min                                                                                                                                                                                                           
│           factor: 0.8                                                                                                                                                                                                         
│           patience: 1                                                                                                                                                                                                         
│           threshold: 0.01                                                                                                                                                                                                     
│           threshold_mode: abs                                                                                                                                                                                                 
│           cooldown: 1                                                                                                                                                                                                         
│           min_lr: 1.0e-06                                                                                                                                                                                                     
│           eps: 1.0e-08                                                                                                                                                                                                        
│           verbose: true                                                                                                                                                                                                       
│         model_z_to_x_scheduler_dict:                                                                                                                                                                                          
│           interval: epoch                                                                                                                                                                                                     
│           frequency: 1                                                                                                                                                                                                        
│           monitor: train/loss                                                                                                                                                                                                 
│           name: LearningRateScheduler_torch.optim.lr_scheduler.ReduceLROnPlateau                                                                                                                                              
│         disc_x_scheduler:                                                                                                                                                                                                     
│           _target_: torch.optim.lr_scheduler.ReduceLROnPlateau                                                                                                                                                                
│           mode: min                                                                                                                                                                                                           
│           factor: 0.8                                                                                                                                                                                                         
│           patience: 1                                                                                                                                                                                                         
│           threshold: 0.01                                                                                                                                                                                                     
│           threshold_mode: abs                                                                                                                                                                                                 
│           cooldown: 1                                                                                                                                                                                                         
│           min_lr: 1.0e-06                                                                                                                                                                                                     
│           eps: 1.0e-08                                                                                                                                                                                                        
│           verbose: true                                                                                                                                                                                                       
│         disc_x_scheduler_dict:                                                                                                                                                                                                
│           interval: epoch                                                                                                                                                                                                     
│           frequency: 1                                                                                                                                                                                                        
│           monitor: train/loss                                                                                                                                                                                                 
│           name: LearningRateScheduler_torch.optim.lr_scheduler.ReduceLROnPlateau                                                                                                                                              
│         disc_z_scheduler:                                                                                                                                                                                                     
│           _target_: torch.optim.lr_scheduler.ReduceLROnPlateau                                                                                                                                                                
│           mode: min                                                                                                                                                                                                           
│           factor: 0.8                                                                                                                                                                                                         
│           patience: 1                                                                                                                                                                                                         
│           threshold: 0.01                                                                                                                                                                                                     
│           threshold_mode: abs                                                                                                                                                                                                 
│           cooldown: 1                                                                                                                                                                                                         
│           min_lr: 1.0e-06                                                                                                                                                                                                     
│           eps: 1.0e-08                                                                                                                                                                                                        
│           verbose: true                                                                                                                                                                                                       
│         disc_z_scheduler_dict:                                                                                                                                                                                                
│           interval: epoch                                                                                                                                                                                                     
│           frequency: 1                                                                                                                                                                                                        
│           monitor: train/loss                                                                                                                                                                                                 
│           name: LearningRateScheduler_torch.optim.lr_scheduler.ReduceLROnPlateau                                                                                                                                              
│       optimizer:                                                                                                                                                                                                              
│         _target_: torch.optim.AdamW                                                                                                                                                                                           
│         lr: 0.001                                                                                                                                                                                                             
│         weight_decay: 0.01                                                                                                                                                                                                    
│         eps: 1.0e-08                                                                                                                                                                                                          
│         betas:                                                                                                                                                                                                                
│         - 0.9                                                                                                                                                                                                                 
│         - 0.999                                                                                                                                                                                                               
│       sequence_to_sequence_model:                                                                                                                                                                                             
│         bert_bert:                                                                                                                                                                                                            
│           _target_: src.models.modules.sequence_to_sequence_models.encoder_decoder.EncoderDecoder                                                                                                                             
│           config_encoder:                                                                                                                                                                                                     
│             _target_: transformers.models.bert.modeling_bert.BertConfig                                                                                                                                                       
│             vocab_size: 20                                                                                                                                                                                                    
│             hidden_size: 128                                                                                                                                                                                                  
│             num_hidden_layers: 2                                                                                                                                                                                              
│             num_attention_heads: 4                                                                                                                                                                                            
│             intermediate_size: 512                                                                                                                                                                                            
│             max_position_embeddings: 100                                                                                                                                                                                      
│             hidden_act: gelu                                                                                                                                                                                                  
│           config_decoder:                                                                                                                                                                                                     
│             _target_: transformers.models.bert.modeling_bert.BertConfig                                                                                                                                                       
│             vocab_size: 20                                                                                                                                                                                                    
│             hidden_size: 128                                                                                                                                                                                                  
│             num_hidden_layers: 2                                                                                                                                                                                              
│             num_attention_heads: 4                                                                                                                                                                                            
│             intermediate_size: 512                                                                                                                                                                                            
│             max_position_embeddings: 100                                                                                                                                                                                      
│             hidden_act: gelu                                                                                                                                                                                                  
│         bert_gpt2:                                                                                                                                                                                                            
│           _target_: to be filled in                                                                                                                                                                                           
│           params:                                                                                                                                                                                                             
│             encoder_params: to be filled in                                                                                                                                                                                   
│             decoder_params: to be filled in                                                                                                                                                                                   
│       _target_: src.models.xz_autoencoder.XZAutoencoder                                                                                                                                                                       
│       name: XZAutoencoder                                                                                                                                                                                                     
│       special_tokens:                                                                                                                                                                                                         
│       - '[pad]'                                                                                                                                                                                                               
│       - '[bos]'                                                                                                                                                                                                               
│       - '[eos]'                                                                                                                                                                                                               
│       - '[unk]'                                                                                                                                                                                                               
│       modules:                                                                                                                                                                                                                
│         model_x_to_z:                                                                                                                                                                                                         
│           _target_: src.models.modules.sequence_to_sequence_models.encoder_decoder.EncoderDecoder                                                                                                                             
│           config_encoder:                                                                                                                                                                                                     
│             _target_: transformers.models.bert.modeling_bert.BertConfig                                                                                                                                                       
│             vocab_size: 20                                                                                                                                                                                                    
│             hidden_size: 128                                                                                                                                                                                                  
│             num_hidden_layers: 2                                                                                                                                                                                              
│             num_attention_heads: 4                                                                                                                                                                                            
│             intermediate_size: 512                                                                                                                                                                                            
│             max_position_embeddings: 100                                                                                                                                                                                      
│             hidden_act: gelu                                                                                                                                                                                                  
│           config_decoder:                                                                                                                                                                                                     
│             _target_: transformers.models.bert.modeling_bert.BertConfig                                                                                                                                                       
│             vocab_size: 20                                                                                                                                                                                                    
│             hidden_size: 128                                                                                                                                                                                                  
│             num_hidden_layers: 2                                                                                                                                                                                              
│             num_attention_heads: 4                                                                                                                                                                                            
│             intermediate_size: 512                                                                                                                                                                                            
│             max_position_embeddings: 100                                                                                                                                                                                      
│             hidden_act: gelu                                                                                                                                                                                                  
│         model_z_to_x:                                                                                                                                                                                                         
│           _target_: src.models.modules.sequence_to_sequence_models.encoder_decoder.EncoderDecoder                                                                                                                             
│           config_encoder:                                                                                                                                                                                                     
│             _target_: transformers.models.bert.modeling_bert.BertConfig                                                                                                                                                       
│             vocab_size: 20                                                                                                                                                                                                    
│             hidden_size: 128                                                                                                                                                                                                  
│             num_hidden_layers: 2                                                                                                                                                                                              
│             num_attention_heads: 4                                                                                                                                                                                            
│             intermediate_size: 512                                                                                                                                                                                            
│             max_position_embeddings: 100                                                                                                                                                                                      
│             hidden_act: gelu                                                                                                                                                                                                  
│           config_decoder:                                                                                                                                                                                                     
│             _target_: transformers.models.bert.modeling_bert.BertConfig                                                                                                                                                       
│             vocab_size: 20                                                                                                                                                                                                    
│             hidden_size: 128                                                                                                                                                                                                  
│             num_hidden_layers: 2                                                                                                                                                                                              
│             num_attention_heads: 4                                                                                                                                                                                            
│             intermediate_size: 512                                                                                                                                                                                            
│             max_position_embeddings: 100                                                                                                                                                                                      
│             hidden_act: gelu                                                                                                                                                                                                  
│         disc_x:                                                                                                                                                                                                               
│           _target_: src.models.modules.discrete_layers.entmax.EntmaxDiscreteLayer                                                                                                                                             
│           alpha: 1.1                                                                                                                                                                                                          
│         disc_z:                                                                                                                                                                                                               
│           _target_: src.models.modules.discrete_layers.entmax.EntmaxDiscreteLayer                                                                                                                                             
│           alpha: 1.1                                                                                                                                                                                                          
│       model_params:                                                                                                                                                                                                           
│         reconstruction_loss_coeff_x: 1.0                                                                                                                                                                                      
│         reconstruction_loss_coeff_z: 1.0                                                                                                                                                                                      
│         tokenize_after_generation: true                                                                                                                                                                                       
│         use_tokenizer_vocab_len: true                                                                                                                                                                                         
│         disc_x_vocab_size: -1                                                                                                                                                                                                 
│         disc_z_vocab_size: -1                                                                                                                                                                                                 
│         max_x_length: 100                                                                                                                                                                                                     
│         max_z_length: 100                                                                                                                                                                                                     
│                                                                                                                                                                                                                               
├── datamodule
│   └── _target_: src.datamodules.scan.SCANDatamodule                                                                                                                                                                           
│       dataset_target_: src.datamodules.scan.SCANDataset                                                                                                                                                                       
│       seed: 42                                                                                                                                                                                                                
│       p_sup: 1.0                                                                                                                                                                                                              
│       dataset_parameters:                                                                                                                                                                                                     
│         seed: 42                                                                                                                                                                                                              
│         batch_size: 4                                                                                                                                                                                                         
│         test_split: simple                                                                                                                                                                                                    
│         train_ratio: 0.8                                                                                                                                                                                                      
│         sup_ratio: 0.02                                                                                                                                                                                                       
│         num_workers: 1                                                                                                                                                                                                        
│         overfit_batch: 0                                                                                                                                                                                                      
│       datasets:                                                                                                                                                                                                               
│         seed: 42                                                                                                                                                                                                              
│         test:                                                                                                                                                                                                                 
│           _target_: src.datamodules.scan.SCANDataset                                                                                                                                                                          
│           split: test                                                                                                                                                                                                         
│         train:                                                                                                                                                                                                                
│           _target_: src.datamodules.scan.SCANDataset                                                                                                                                                                          
│           split: train                                                                                                                                                                                                        
│         val:                                                                                                                                                                                                                  
│           _target_: src.datamodules.scan.SCANDataset                                                                                                                                                                          
│           split: val                                                                                                                                                                                                          
│                                                                                                                                                                                                                               
├── callbacks
│   └── model_checkpoint:                                                                                                                                                                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                                                                                                                                                                 
│         monitor: val/loss                                                                                                                                                                                                     
│         mode: min                                                                                                                                                                                                             
│         save_top_k: 3                                                                                                                                                                                                         
│         save_last: true                                                                                                                                                                                                       
│         verbose: false                                                                                                                                                                                                        
│         dirpath: checkpoints/                                                                                                                                                                                                 
│         filename: model-{step:04d}-{val/loss:.4f}                                                                                                                                                                             
│         save_on_train_epoch_end: false                                                                                                                                                                                        
│         auto_insert_metric_name: false                                                                                                                                                                                        
│       learning_rate_monitor:                                                                                                                                                                                                  
│         _target_: pytorch_lightning.callbacks.LearningRateMonitor                                                                                                                                                             
│         logging_interval: step                                                                                                                                                                                                
│       early_stopping:                                                                                                                                                                                                         
│         _target_: pytorch_lightning.callbacks.EarlyStopping                                                                                                                                                                   
│         monitor: val/loss                                                                                                                                                                                                     
│         mode: min                                                                                                                                                                                                             
│         patience: 200                                                                                                                                                                                                         
│         min_delta: 0                                                                                                                                                                                                          
│       z_supervised_scheduler:                                                                                                                                                                                                 
│         _target_: src.callbacks.scheduler_callback.SchedulerCallback                                                                                                                                                          
│         hyperparameter_location: trainer.datamodule.train_sampler.p_sup                                                                                                                                                       
│         hyperparameter_location_pl: model_params.prob_z_sup                                                                                                                                                                   
│         scheduler:                                                                                                                                                                                                            
│           _target_: src.schedulers.linear_scheduler.LinearScheduler                                                                                                                                                           
│           num_warmup_steps: 25                                                                                                                                                                                                
│           num_training_steps: 80                                                                                                                                                                                              
│           hp_init: 1.0                                                                                                                                                                                                        
│           hp_end: 0.0                                                                                                                                                                                                         
│           power: 1.0                                                                                                                                                                                                          
│                                                                                                                                                                                                                               
├── debug
│   └── None                                                                                                                                                                                                                    
├── seed
│   └── 42                                                                                                                                                                                                                      
├── run_name
│   └── supervised                                                                                                                                                                                                              
├── ignore_warnings
│   └── False                                                                                                                                                                                                                   
└── test_after_training
    └── None                                                                                                                                                                                                                    
