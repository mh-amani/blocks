callbacks:
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: val/loss
    mode: min
    save_top_k: 3
    save_last: true
    verbose: false
    dirpath: checkpoints/
    filename: model-{step:04d}-{val/loss:.4f}
    save_on_train_epoch_end: false
    auto_insert_metric_name: false
  learning_rate_monitor:
    _target_: pytorch_lightning.callbacks.LearningRateMonitor
    logging_interval: step
  early_stopping:
    _target_: pytorch_lightning.callbacks.EarlyStopping
    monitor: val/loss
    mode: min
    patience: 200
    min_delta: 0
  probability_logger_wandb:
    _target_: src.callbacks.wandb_output_logger.ProbabilityLogger
  z_supervised_scheduler:
    _target_: src.callbacks.scheduler_callback.SchedulerCallback
    hyperparameter_location: trainer.datamodule.train_sampler.p_sup
    hyperparameter_location_pl: model_params.prob_z_sup
    scheduler:
      _target_: src.schedulers.linear_scheduler.LinearScheduler
      num_warmup_steps: 25
      num_training_steps: 80
      hp_init: 1.0
      hp_end: 0.0
      power: 1.0
datamodule:
  _target_: src.datamodules.scan.SCANDatamodule
  dataset_target_: src.datamodules.scan.SCANDataset
  seed: 42
  p_sup: callback.z_supervised_scheduler.scheduler.hp_init
  dataset_parameters:
    seed: ${seed}
    batch_size: 128
    test_split: simple
    train_ratio: 0.8
    sup_ratio: 0.1
    num_workers: 1
    overfit_batch: ${overfit_batch}
  datasets:
    seed: ${seed}
    test:
      _target_: ${datamodule.dataset_target_}
      split: test
    train:
      _target_: ${datamodule.dataset_target_}
      split: train
    val:
      _target_: ${datamodule.dataset_target_}
      split: val
  sup_ratio: 0.02
  batch_size: 128
  num_workers: 1
model:
  discretizer:
    entmax:
      _target_: src.models.modules.discrete_layers.entmax.EntmaxDiscreteLayer
      alpha: 1.1
    gumbel:
      _target_: src.models.modules.discrete_layers.gumbel.Gumbel
      params: null
  tokenizer:
    __target__: src.models.tokenizer.simple_unigram_tokenizer.SimpleUnigramTokenizer
    model_max_length: 100
    padding_side: right
    vocab_size: 8000
    show_progress: true
    shrinking_factor: 0.75
    max_piece_length: 16
    n_sub_iterations: 2
    special_tokens:
    - '[pad]'
    - '[bos]'
    - '[eos]'
    - '[UNK]'
    batch_size: ${datamodule.batch_size}
  collator: {}
  lr_scheduler:
    _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
    mode: min
    factor: 0.8
    patience: 1
    threshold: 0.01
    threshold_mode: abs
    cooldown: 1
    min_lr: 1.0e-06
    eps: 1.0e-08
    verbose: true
    interval: epoch
    frequency: 1
    monitor: train/loss
    model_x_to_z_scheduler:
      _target_: ${model.lr_scheduler._target_}
      mode: ${model.lr_scheduler.mode}
      factor: ${model.lr_scheduler.factor}
      patience: ${model.lr_scheduler.patience}
      threshold: ${model.lr_scheduler.threshold}
      threshold_mode: ${model.lr_scheduler.threshold_mode}
      cooldown: ${model.lr_scheduler.cooldown}
      min_lr: ${model.lr_scheduler.min_lr}
      eps: ${model.lr_scheduler.eps}
      verbose: ${model.lr_scheduler.verbose}
    model_x_to_z_scheduler_dict:
      interval: ${model.lr_scheduler.interval}
      frequency: ${model.lr_scheduler.frequency}
      monitor: ${model.lr_scheduler.monitor}
      name: LearningRateScheduler_${model.lr_scheduler.model_x_to_z_scheduler._target_}
    model_z_to_x_scheduler:
      _target_: ${model.lr_scheduler._target_}
      mode: ${model.lr_scheduler.mode}
      factor: ${model.lr_scheduler.factor}
      patience: ${model.lr_scheduler.patience}
      threshold: ${model.lr_scheduler.threshold}
      threshold_mode: ${model.lr_scheduler.threshold_mode}
      cooldown: ${model.lr_scheduler.cooldown}
      min_lr: ${model.lr_scheduler.min_lr}
      eps: ${model.lr_scheduler.eps}
      verbose: ${model.lr_scheduler.verbose}
    model_z_to_x_scheduler_dict:
      interval: ${model.lr_scheduler.interval}
      frequency: ${model.lr_scheduler.frequency}
      monitor: ${model.lr_scheduler.monitor}
      name: LearningRateScheduler_${model.lr_scheduler.model_z_to_x_scheduler._target_}
    disc_x_scheduler:
      _target_: ${model.lr_scheduler._target_}
      mode: ${model.lr_scheduler.mode}
      factor: ${model.lr_scheduler.factor}
      patience: ${model.lr_scheduler.patience}
      threshold: ${model.lr_scheduler.threshold}
      threshold_mode: ${model.lr_scheduler.threshold_mode}
      cooldown: ${model.lr_scheduler.cooldown}
      min_lr: ${model.lr_scheduler.min_lr}
      eps: ${model.lr_scheduler.eps}
      verbose: ${model.lr_scheduler.verbose}
    disc_x_scheduler_dict:
      interval: ${model.lr_scheduler.interval}
      frequency: ${model.lr_scheduler.frequency}
      monitor: ${model.lr_scheduler.monitor}
      name: LearningRateScheduler_${model.lr_scheduler.disc_x_scheduler._target_}
    disc_z_scheduler:
      _target_: ${model.lr_scheduler._target_}
      mode: ${model.lr_scheduler.mode}
      factor: ${model.lr_scheduler.factor}
      patience: ${model.lr_scheduler.patience}
      threshold: ${model.lr_scheduler.threshold}
      threshold_mode: ${model.lr_scheduler.threshold_mode}
      cooldown: ${model.lr_scheduler.cooldown}
      min_lr: ${model.lr_scheduler.min_lr}
      eps: ${model.lr_scheduler.eps}
      verbose: ${model.lr_scheduler.verbose}
    disc_z_scheduler_dict:
      interval: ${model.lr_scheduler.interval}
      frequency: ${model.lr_scheduler.frequency}
      monitor: ${model.lr_scheduler.monitor}
      name: LearningRateScheduler_${model.lr_scheduler.disc_z_scheduler._target_}
  optimizer:
    _target_: torch.optim.AdamW
    lr: 0.001
    weight_decay: 0.01
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
  sequence_to_sequence_model:
    bert_bert:
      _target_: src.models.modules.sequence_to_sequence_models.encoder_decoder.EncoderDecoder
      config_encoder:
        _target_: transformers.models.bert.modeling_bert.BertConfig
        vocab_size: 20
        hidden_size: 128
        num_hidden_layers: 2
        num_attention_heads: 4
        intermediate_size: 512
        max_position_embeddings: 100
        hidden_act: gelu
        pad_token_id: ${pad_token_id}
      config_decoder:
        _target_: transformers.models.bert.modeling_bert.BertConfig
        vocab_size: 20
        hidden_size: 128
        num_hidden_layers: 2
        num_attention_heads: 4
        intermediate_size: 512
        max_position_embeddings: 100
        hidden_act: gelu
        pad_token_id: ${pad_token_id}
    bert_gpt2:
      _target_: to be filled in
      params:
        encoder_params: to be filled in
        decoder_params: to be filled in
  _target_: src.models.xz_autoencoder.XZAutoencoder
  name: XZAutoencoder
  pad_token_id: 0
  bos_token_id: 1
  eos_token_id: 2
  modules:
    model_x_to_z: ${model.sequence_to_sequence_model.bert_bert}
    model_z_to_x: ${model.sequence_to_sequence_model.bert_bert}
    disc_x: ${model.discretizer.entmax}
    disc_z: ${model.discretizer.entmax}
    tokenizer_x: ${model.tokenizer}
    tokenizer_z: ${model.tokenizer}
  model_params:
    reconstruction_loss_coeff_x: 1.0
    reconstruction_loss_coeff_z: 1.0
trainer:
  _target_: pytorch_lightning.Trainer
  devices: auto
  accelerator: auto
  accumulate_grad_batches: 1
  max_epochs: 1000
  min_epochs: 10
  max_steps: -1
  check_val_every_n_epoch: 1
  fast_dev_run: false
logger:
  wandb:
    _target_: pytorch_lightning.loggers.wandb.WandbLogger
    project: ${name}
    name: ${run_name}
    save_dir: .
    offline: false
    id: null
    entity: null
    log_model: false
    job_type: train
    group: ''
    tags: []
    notes: null
resume_from_checkpoint: null
pad_token_id: 0
work_dir: ${hydra:runtime.cwd}
data_dir: ${work_dir}/data
output_dir: ${hydra:runtime.output_dir}
ignore_warnings: false
print_config: true
seed: 42
logs_subfolder: train
run_name: supervised
name: scan_symae
track_gradients: true
overfit_batch: 0
