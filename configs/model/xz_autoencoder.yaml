_target_: src.models.xz_autoencoder.XZAutoencoder
name: "XZAutoencoder"

defaults:
  # - inference: default
  - discretizer:
    - entmax
    - gumbel
  - collator: simple_text_collator
  - lr_scheduler: reduce_on_plateau # options: 
  - optimizer: default # options: default (Adam), AdamW
  - sequence_to_sequence_model:
    - bert_bert
    - bert_gpt2
    - gpt2_gpt2
    - bart

special_tokens: ["[pad]", "[bos]", "[eos]", "[unk]"]


modules:
  model_x_to_z: ???
  model_z_to_x: ???
  disc_x: ???
  disc_z: ???

model_params:
  loss_coeff:
    xzx: 1.0
    zxz: 1.0
    supervised_seperated_x: 1.0
    supervised_seperated_z: 1.0


  tokenize_after_generation: true
  
  acc_grad_batch: 1

  use_tokenizer_vocab_len: true
  disc_x_vocab_size: -1
  disc_z_vocab_size: -1
  max_x_length: 100
  max_z_length: 100