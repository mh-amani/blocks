bert_bert:
    _target_: src.models.modules.sequence_to_sequence_models.encoder_decoder.EncoderDecoder

    config_encoder:
        _target_: transformers.models.bert.modeling_bert.BertConfig
        # Maximum sequence length that this model might ever be used with
    
        vocab_size: 20
        hidden_size: 128
        num_hidden_layers: 2
        num_attention_heads: 4
        intermediate_size: 512
        max_position_embeddings: 100
        hidden_act: 'gelu'
        

    config_decoder:
        _target_: transformers.models.bert.modeling_bert.BertConfig
        vocab_size: 20
        hidden_size: 128
        num_hidden_layers: 2
        num_attention_heads: 4
        intermediate_size: 512
        max_position_embeddings: 100
        hidden_act: 'gelu'