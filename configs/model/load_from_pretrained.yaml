_target_: src.models.xz_autoencoder.XZAutoencoder.load_from_checkpoint
checkpoint_path: ???

defaults:
  - collator: loaded_collator

model_params:
    tokenize_prior_training: ${model.substitute_config.model_params.tokenize_prior_training}
    decode_after_autoreg_step: ${model.substitute_config.model_params.decode_after_autoreg_step}
    max_x_length: ${model.substitute_config.model_params.max_x_length}
    max_x_vocab_size: ${model.substitute_config.model_params.max_x_vocab_size}
    max_z_length: ${model.substitute_config.model_params.max_z_length}
    max_z_vocab_size: ${model.substitute_config.model_params.max_z_vocab_size}

substitute_config:  
  collator: ${model.collator}

  model_params:
    tokenize_prior_training: True
    decode_after_autoreg_step: True
    max_x_length: 100
    max_x_vocab_size: 200
    max_z_length: 100
    max_z_vocab_size: 200

    loss_coeff:
      xzx: 1.0
      zxz: 1.0
      supervised_seperated_x: 1.0
      supervised_seperated_z: 1.0
    
    acc_grad_batch: 1

    use_tokenizer_vocab_len: true
    disc_x_vocab_size: -1
    disc_z_vocab_size: -1

  optimizer:
    lr: 0.002232423

  lr_scheduler:
    mode: "min"
    factor: 0.8
    patience: 5
    threshold: 0.01
    threshold_mode: "abs"
    cooldown: 5
    min_lr: 1e-6
    eps: 1e-8
    verbose: True
  