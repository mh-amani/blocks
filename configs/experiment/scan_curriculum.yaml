# @package _global_

defaults:
  - override /datamodule: scan.yaml
  - override /model: load_from_pretrained.yaml

# name of the run determines folder name in logs
name: "scan_sigmae"
run_name: "curriculum-${datamodule.dataset_parameters.supervision_ratio}-${sequence_to_sequence_model_key}-${discretizer_key}" 

sequence_to_sequence_model_key: ???
discretizer_key: ???

track_gradients: Yes
overfit_batch: 0

num_epochs: 1000

trainer:
  devices: [0] # 'auto', or numbers like 2, [0]
  accelerator: 'gpu' #cpu, tpu, (devices=4, accelerator="gpu", strategy="ddp"), (devices="auto", accelerator="auto")  
  max_epochs: ${num_epochs}
  min_epochs: ${num_epochs}

datamodule:
  dataset_parameters:
    supervision_ratio: [0.02, 0.9] # [r(xz), r(z|not xz)]
    batch_size: 128
    num_workers: 8
    remove_long_data_points: False

model:
  checkpoint_path: ???
  substitute_config:
    model_params:
      use_pc_grad: False

      # for gradient inner product logging in val step 
      log_gradient_stats: False
      num_steps_log_gradient_stats: 8
      log_gradient_stats_batch_size: 32

      acc_grad_batch: 1
      num_bootstrap_tests: 10
      
      usexz: True
      usex: False
      usez: False

      max_x_length: 20
      max_x_vocab_size: 25
      max_z_length: 60
      max_z_vocab_size: 25
      
      loss_coeff:
        xzx: 1.0
        zxz: 1.0
        supervised_seperated_x: 1.0
        supervised_seperated_z: 1.0
        quantization_zxz: 0.0                                                                                                                                                                                                                                                                                                        
        quantization_xzx: 0.0
        quantization_supervised_seperated: 0.0


    optimizer:
      lr: 0.001

    lr_scheduler: # ${model.lr_scheduler}
      mode: "min"
      factor: 0.98
      patience: 5
      threshold: 0.01
      threshold_mode: "abs"
      cooldown: 5
      min_lr: 1e-6
      eps: 1e-8
      verbose: True

callbacks:
  supervision_scheduler:
    scheduler_xz:
      num_warmup_steps: 10
      num_training_steps: 300
      hp_init: 1.0
      hp_end: 0.5
    scheduler_z:
      # between 0 and 1, saying given that a sample is not xz, how likely is it to be z.
      num_warmup_steps: 300
      num_training_steps: 500
      hp_init: 1.0
      hp_end: 0.8


logger:
  wandb:
    tags: ["supervised-training"]
    notes: