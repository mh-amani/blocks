wandb_version: 1

_wandb:
  desc: null
  value:
    python_version: 3.11.4
    cli_version: 0.15.8
    framework: huggingface
    huggingface_version: 4.32.0
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1693904010.087392
    t:
      1:
      - 1
      - 5
      - 9
      - 11
      - 49
      - 50
      - 51
      - 53
      - 55
      - 103
      2:
      - 1
      - 5
      - 9
      - 11
      - 49
      - 50
      - 51
      - 53
      - 55
      - 103
      3:
      - 1
      - 7
      - 13
      - 23
      4: 3.11.4
      5: 0.15.8
      6: 4.32.0
      8:
      - 4
      - 5
    m:
    - 1: trainer/global_step
      6:
      - 3
trainer:
  desc: null
  value: '{''_target_'': ''pytorch_lightning.Trainer'', ''devices'': ''auto'', ''accelerator'':
    ''auto'', ''accumulate_grad_batches'': 1, ''max_epochs'': 1000, ''min_epochs'':
    10, ''max_steps'': -1, ''check_val_every_n_epoch'': 1, ''fast_dev_run'': False}'
model:
  desc: null
  value: '{''discretizer'': {''entmax'': {''_target_'': ''src.models.modules.discrete_layers.entmax.EntmaxDiscreteLayer'',
    ''alpha'': 1.1}, ''gumbel'': {''_target_'': ''src.models.modules.discrete_layers.gumbel.Gumbel'',
    ''params'': None}}, ''collator'': {''tokenizer'': {''_target_'': ''src.models.collators.tokenizers.simple_word_level_tokenizer.SimpleWordLevelTokenizer'',
    ''model_max_length'': ''${model.collator.max_length}'', ''vocab_size'': 30, ''special_tokens'':
    ''${model.collator.special_tokens}'', ''pad_token_id'': ''${model.collator.pad_token_id}'',
    ''batch_size'': ''${datamodule.batch_size}''}, ''_target_'': ''src.models.collators.simple_text_collator.SimpleTextCollator'',
    ''max_length'': 100, ''padding_side'': ''right'', ''special_tokens'': [''[pad]'',
    ''[bos]'', ''[eos]'', ''[UNK]''], ''pad_token_id'': 0, ''max_X_length'': ''${model.collator.max_length}'',
    ''max_Z_length'': ''${model.collator.max_length}'', ''padding'': True}, ''lr_scheduler'':
    {''_target_'': ''torch.optim.lr_scheduler.ReduceLROnPlateau'', ''mode'': ''min'',
    ''factor'': 0.8, ''patience'': 1, ''threshold'': 0.01, ''threshold_mode'': ''abs'',
    ''cooldown'': 1, ''min_lr'': 1e-06, ''eps'': 1e-08, ''verbose'': True, ''interval'':
    ''epoch'', ''frequency'': 1, ''monitor'': ''train/loss'', ''model_x_to_z_scheduler'':
    {''_target_'': ''${model.lr_scheduler._target_}'', ''mode'': ''${model.lr_scheduler.mode}'',
    ''factor'': ''${model.lr_scheduler.factor}'', ''patience'': ''${model.lr_scheduler.patience}'',
    ''threshold'': ''${model.lr_scheduler.threshold}'', ''threshold_mode'': ''${model.lr_scheduler.threshold_mode}'',
    ''cooldown'': ''${model.lr_scheduler.cooldown}'', ''min_lr'': ''${model.lr_scheduler.min_lr}'',
    ''eps'': ''${model.lr_scheduler.eps}'', ''verbose'': ''${model.lr_scheduler.verbose}''},
    ''model_x_to_z_scheduler_dict'': {''interval'': ''${model.lr_scheduler.interval}'',
    ''frequency'': ''${model.lr_scheduler.frequency}'', ''monitor'': ''${model.lr_scheduler.monitor}'',
    ''name'': ''LearningRateScheduler_${model.lr_scheduler.model_x_to_z_scheduler._target_}''},
    ''model_z_to_x_scheduler'': {''_target_'': ''${model.lr_scheduler._target_}'',
    ''mode'': ''${model.lr_scheduler.mode}'', ''factor'': ''${model.lr_scheduler.factor}'',
    ''patience'': ''${model.lr_scheduler.patience}'', ''threshold'': ''${model.lr_scheduler.threshold}'',
    ''threshold_mode'': ''${model.lr_scheduler.threshold_mode}'', ''cooldown'': ''${model.lr_scheduler.cooldown}'',
    ''min_lr'': ''${model.lr_scheduler.min_lr}'', ''eps'': ''${model.lr_scheduler.eps}'',
    ''verbose'': ''${model.lr_scheduler.verbose}''}, ''model_z_to_x_scheduler_dict'':
    {''interval'': ''${model.lr_scheduler.interval}'', ''frequency'': ''${model.lr_scheduler.frequency}'',
    ''monitor'': ''${model.lr_scheduler.monitor}'', ''name'': ''LearningRateScheduler_${model.lr_scheduler.model_z_to_x_scheduler._target_}''},
    ''disc_x_scheduler'': {''_target_'': ''${model.lr_scheduler._target_}'', ''mode'':
    ''${model.lr_scheduler.mode}'', ''factor'': ''${model.lr_scheduler.factor}'',
    ''patience'': ''${model.lr_scheduler.patience}'', ''threshold'': ''${model.lr_scheduler.threshold}'',
    ''threshold_mode'': ''${model.lr_scheduler.threshold_mode}'', ''cooldown'': ''${model.lr_scheduler.cooldown}'',
    ''min_lr'': ''${model.lr_scheduler.min_lr}'', ''eps'': ''${model.lr_scheduler.eps}'',
    ''verbose'': ''${model.lr_scheduler.verbose}''}, ''disc_x_scheduler_dict'': {''interval'':
    ''${model.lr_scheduler.interval}'', ''frequency'': ''${model.lr_scheduler.frequency}'',
    ''monitor'': ''${model.lr_scheduler.monitor}'', ''name'': ''LearningRateScheduler_${model.lr_scheduler.disc_x_scheduler._target_}''},
    ''disc_z_scheduler'': {''_target_'': ''${model.lr_scheduler._target_}'', ''mode'':
    ''${model.lr_scheduler.mode}'', ''factor'': ''${model.lr_scheduler.factor}'',
    ''patience'': ''${model.lr_scheduler.patience}'', ''threshold'': ''${model.lr_scheduler.threshold}'',
    ''threshold_mode'': ''${model.lr_scheduler.threshold_mode}'', ''cooldown'': ''${model.lr_scheduler.cooldown}'',
    ''min_lr'': ''${model.lr_scheduler.min_lr}'', ''eps'': ''${model.lr_scheduler.eps}'',
    ''verbose'': ''${model.lr_scheduler.verbose}''}, ''disc_z_scheduler_dict'': {''interval'':
    ''${model.lr_scheduler.interval}'', ''frequency'': ''${model.lr_scheduler.frequency}'',
    ''monitor'': ''${model.lr_scheduler.monitor}'', ''name'': ''LearningRateScheduler_${model.lr_scheduler.disc_z_scheduler._target_}''}},
    ''optimizer'': {''_target_'': ''torch.optim.AdamW'', ''lr'': 0.001, ''weight_decay'':
    0.01, ''eps'': 1e-08, ''betas'': [0.9, 0.999]}, ''sequence_to_sequence_model'':
    {''bert_bert'': {''_target_'': ''src.models.modules.sequence_to_sequence_models.encoder_decoder.EncoderDecoder'',
    ''config_encoder'': {''_target_'': ''transformers.models.bert.modeling_bert.BertConfig'',
    ''vocab_size'': 20, ''hidden_size'': 128, ''num_hidden_layers'': 2, ''num_attention_heads'':
    4, ''intermediate_size'': 512, ''max_position_embeddings'': 100, ''hidden_act'':
    ''gelu'', ''pad_token_id'': 0}, ''config_decoder'': {''_target_'': ''transformers.models.bert.modeling_bert.BertConfig'',
    ''vocab_size'': 20, ''hidden_size'': 128, ''num_hidden_layers'': 2, ''num_attention_heads'':
    4, ''intermediate_size'': 512, ''max_position_embeddings'': 100, ''hidden_act'':
    ''gelu'', ''pad_token_id'': 0}}, ''bert_gpt2'': {''_target_'': ''to be filled
    in'', ''params'': {''encoder_params'': ''to be filled in'', ''decoder_params'':
    ''to be filled in''}}}, ''_target_'': ''src.models.xz_autoencoder.XZAutoencoder'',
    ''name'': ''XZAutoencoder'', ''pad_token_id'': 0, ''bos_token_id'': 1, ''eos_token_id'':
    2, ''modules'': {''model_x_to_z'': ''${model.sequence_to_sequence_model.bert_bert}'',
    ''model_z_to_x'': ''${model.sequence_to_sequence_model.bert_bert}'', ''disc_x'':
    ''${model.discretizer.entmax}'', ''disc_z'': ''${model.discretizer.entmax}''},
    ''model_params'': {''reconstruction_loss_coeff_x'': 1.0, ''reconstruction_loss_coeff_z'':
    1.0}}'
datamodule:
  desc: null
  value: <src.datamodules.scan.SCANDatamodule object at 0x297537d90>
model/params_total:
  desc: null
  value: 2014504
model/params_trainable:
  desc: null
  value: 2014504
model/params_not_trainable:
  desc: null
  value: 0
rel_path_to_work_dir:
  desc: null
  value: .
seed:
  desc: null
  value: 42
callbacks:
  desc: null
  value: '{''model_checkpoint'': {''_target_'': ''pytorch_lightning.callbacks.ModelCheckpoint'',
    ''monitor'': ''val/loss'', ''mode'': ''min'', ''save_top_k'': 3, ''save_last'':
    True, ''verbose'': False, ''dirpath'': ''checkpoints/'', ''filename'': ''model-{step:04d}-{val/loss:.4f}'',
    ''save_on_train_epoch_end'': False, ''auto_insert_metric_name'': False}, ''learning_rate_monitor'':
    {''_target_'': ''pytorch_lightning.callbacks.LearningRateMonitor'', ''logging_interval'':
    ''step''}, ''early_stopping'': {''_target_'': ''pytorch_lightning.callbacks.EarlyStopping'',
    ''monitor'': ''val/loss'', ''mode'': ''min'', ''patience'': 200, ''min_delta'':
    0}, ''probability_logger_wandb'': {''_target_'': ''src.callbacks.wandb_output_logger.ProbabilityLogger''},
    ''z_supervised_scheduler'': {''_target_'': ''src.callbacks.scheduler_callback.SchedulerCallback'',
    ''hyperparameter_location'': ''trainer.datamodule.train_sampler.p_sup'', ''hyperparameter_location_pl'':
    ''model_params.prob_z_sup'', ''scheduler'': {''_target_'': ''src.schedulers.linear_scheduler.LinearScheduler'',
    ''num_warmup_steps'': 25, ''num_training_steps'': 80, ''hp_init'': 1.0, ''hp_end'':
    0.0, ''power'': 1.0}}}'
discretizer:
  desc: null
  value: '{''entmax'': {''_target_'': ''src.models.modules.discrete_layers.entmax.EntmaxDiscreteLayer'',
    ''alpha'': 1.1}, ''gumbel'': {''_target_'': ''src.models.modules.discrete_layers.gumbel.Gumbel'',
    ''params'': None}}'
collator:
  desc: null
  value: '{''tokenizer'': {''_target_'': ''src.models.collators.tokenizers.simple_word_level_tokenizer.SimpleWordLevelTokenizer'',
    ''model_max_length'': 100, ''vocab_size'': 30, ''special_tokens'': [''[pad]'',
    ''[bos]'', ''[eos]'', ''[UNK]''], ''pad_token_id'': 0, ''batch_size'': 128}, ''_target_'':
    ''src.models.collators.simple_text_collator.SimpleTextCollator'', ''max_length'':
    100, ''padding_side'': ''right'', ''special_tokens'': [''[pad]'', ''[bos]'', ''[eos]'',
    ''[UNK]''], ''pad_token_id'': 0, ''max_X_length'': 100, ''max_Z_length'': 100,
    ''padding'': True}'
lr_scheduler:
  desc: null
  value: '{''_target_'': ''torch.optim.lr_scheduler.ReduceLROnPlateau'', ''mode'':
    ''min'', ''factor'': 0.8, ''patience'': 1, ''threshold'': 0.01, ''threshold_mode'':
    ''abs'', ''cooldown'': 1, ''min_lr'': 1e-06, ''eps'': 1e-08, ''verbose'': True,
    ''interval'': ''epoch'', ''frequency'': 1, ''monitor'': ''train/loss'', ''model_x_to_z_scheduler'':
    {''_target_'': ''torch.optim.lr_scheduler.ReduceLROnPlateau'', ''mode'': ''min'',
    ''factor'': 0.8, ''patience'': 1, ''threshold'': 0.01, ''threshold_mode'': ''abs'',
    ''cooldown'': 1, ''min_lr'': 1e-06, ''eps'': 1e-08, ''verbose'': True}, ''model_x_to_z_scheduler_dict'':
    {''interval'': ''epoch'', ''frequency'': 1, ''monitor'': ''train/loss'', ''name'':
    ''LearningRateScheduler_torch.optim.lr_scheduler.ReduceLROnPlateau''}, ''model_z_to_x_scheduler'':
    {''_target_'': ''torch.optim.lr_scheduler.ReduceLROnPlateau'', ''mode'': ''min'',
    ''factor'': 0.8, ''patience'': 1, ''threshold'': 0.01, ''threshold_mode'': ''abs'',
    ''cooldown'': 1, ''min_lr'': 1e-06, ''eps'': 1e-08, ''verbose'': True}, ''model_z_to_x_scheduler_dict'':
    {''interval'': ''epoch'', ''frequency'': 1, ''monitor'': ''train/loss'', ''name'':
    ''LearningRateScheduler_torch.optim.lr_scheduler.ReduceLROnPlateau''}, ''disc_x_scheduler'':
    {''_target_'': ''torch.optim.lr_scheduler.ReduceLROnPlateau'', ''mode'': ''min'',
    ''factor'': 0.8, ''patience'': 1, ''threshold'': 0.01, ''threshold_mode'': ''abs'',
    ''cooldown'': 1, ''min_lr'': 1e-06, ''eps'': 1e-08, ''verbose'': True}, ''disc_x_scheduler_dict'':
    {''interval'': ''epoch'', ''frequency'': 1, ''monitor'': ''train/loss'', ''name'':
    ''LearningRateScheduler_torch.optim.lr_scheduler.ReduceLROnPlateau''}, ''disc_z_scheduler'':
    {''_target_'': ''torch.optim.lr_scheduler.ReduceLROnPlateau'', ''mode'': ''min'',
    ''factor'': 0.8, ''patience'': 1, ''threshold'': 0.01, ''threshold_mode'': ''abs'',
    ''cooldown'': 1, ''min_lr'': 1e-06, ''eps'': 1e-08, ''verbose'': True}, ''disc_z_scheduler_dict'':
    {''interval'': ''epoch'', ''frequency'': 1, ''monitor'': ''train/loss'', ''name'':
    ''LearningRateScheduler_torch.optim.lr_scheduler.ReduceLROnPlateau''}}'
optimizer:
  desc: null
  value: '{''_target_'': ''torch.optim.AdamW'', ''lr'': 0.001, ''weight_decay'': 0.01,
    ''eps'': 1e-08, ''betas'': [0.9, 0.999]}'
sequence_to_sequence_model:
  desc: null
  value: '{''bert_bert'': {''_target_'': ''src.models.modules.sequence_to_sequence_models.encoder_decoder.EncoderDecoder'',
    ''config_encoder'': {''_target_'': ''transformers.models.bert.modeling_bert.BertConfig'',
    ''vocab_size'': 20, ''hidden_size'': 128, ''num_hidden_layers'': 2, ''num_attention_heads'':
    4, ''intermediate_size'': 512, ''max_position_embeddings'': 100, ''hidden_act'':
    ''gelu'', ''pad_token_id'': 0}, ''config_decoder'': {''_target_'': ''transformers.models.bert.modeling_bert.BertConfig'',
    ''vocab_size'': 20, ''hidden_size'': 128, ''num_hidden_layers'': 2, ''num_attention_heads'':
    4, ''intermediate_size'': 512, ''max_position_embeddings'': 100, ''hidden_act'':
    ''gelu'', ''pad_token_id'': 0}}, ''bert_gpt2'': {''_target_'': ''to be filled
    in'', ''params'': {''encoder_params'': ''to be filled in'', ''decoder_params'':
    ''to be filled in''}}}'
name:
  desc: null
  value: XZAutoencoder
pad_token_id:
  desc: null
  value: 0
bos_token_id:
  desc: null
  value: 1
eos_token_id:
  desc: null
  value: 2
modules:
  desc: null
  value: '{''model_x_to_z'': {''_target_'': ''src.models.modules.sequence_to_sequence_models.encoder_decoder.EncoderDecoder'',
    ''config_encoder'': {''_target_'': ''transformers.models.bert.modeling_bert.BertConfig'',
    ''vocab_size'': 20, ''hidden_size'': 128, ''num_hidden_layers'': 2, ''num_attention_heads'':
    4, ''intermediate_size'': 512, ''max_position_embeddings'': 100, ''hidden_act'':
    ''gelu'', ''pad_token_id'': 0}, ''config_decoder'': {''_target_'': ''transformers.models.bert.modeling_bert.BertConfig'',
    ''vocab_size'': 20, ''hidden_size'': 128, ''num_hidden_layers'': 2, ''num_attention_heads'':
    4, ''intermediate_size'': 512, ''max_position_embeddings'': 100, ''hidden_act'':
    ''gelu'', ''pad_token_id'': 0}}, ''model_z_to_x'': {''_target_'': ''src.models.modules.sequence_to_sequence_models.encoder_decoder.EncoderDecoder'',
    ''config_encoder'': {''_target_'': ''transformers.models.bert.modeling_bert.BertConfig'',
    ''vocab_size'': 20, ''hidden_size'': 128, ''num_hidden_layers'': 2, ''num_attention_heads'':
    4, ''intermediate_size'': 512, ''max_position_embeddings'': 100, ''hidden_act'':
    ''gelu'', ''pad_token_id'': 0}, ''config_decoder'': {''_target_'': ''transformers.models.bert.modeling_bert.BertConfig'',
    ''vocab_size'': 20, ''hidden_size'': 128, ''num_hidden_layers'': 2, ''num_attention_heads'':
    4, ''intermediate_size'': 512, ''max_position_embeddings'': 100, ''hidden_act'':
    ''gelu'', ''pad_token_id'': 0}}, ''disc_x'': {''_target_'': ''src.models.modules.discrete_layers.entmax.EntmaxDiscreteLayer'',
    ''alpha'': 1.1}, ''disc_z'': {''_target_'': ''src.models.modules.discrete_layers.entmax.EntmaxDiscreteLayer'',
    ''alpha'': 1.1}}'
model_params:
  desc: null
  value: '{''reconstruction_loss_coeff_x'': 1.0, ''reconstruction_loss_coeff_z'':
    1.0}'
