{"format": "torch", "nodes": [{"name": "model_x_to_z", "id": 10825369936, "class_name": "EncoderDecoderModel(\n  (encoder): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(20, 128, padding_idx=0)\n      (position_embeddings): Embedding(100, 128)\n      (token_type_embeddings): Embedding(2, 128)\n      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-1): 2 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=128, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=128, out_features=128, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (decoder): BertLMHeadModel(\n    (bert): BertModel(\n      (embeddings): BertEmbeddings(\n        (word_embeddings): Embedding(20, 128, padding_idx=0)\n        (position_embeddings): Embedding(100, 128)\n        (token_type_embeddings): Embedding(2, 128)\n        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (encoder): BertEncoder(\n        (layer): ModuleList(\n          (0-1): 2 x BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=128, out_features=128, bias=True)\n                (key): Linear(in_features=128, out_features=128, bias=True)\n                (value): Linear(in_features=128, out_features=128, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=128, out_features=128, bias=True)\n                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (crossattention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=128, out_features=128, bias=True)\n                (key): Linear(in_features=128, out_features=128, bias=True)\n                (value): Linear(in_features=128, out_features=128, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=128, out_features=128, bias=True)\n                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n    )\n    (cls): BertOnlyMLMHead(\n      (predictions): BertLMPredictionHead(\n        (transform): BertPredictionHeadTransform(\n          (dense): Linear(in_features=128, out_features=128, bias=True)\n          (transform_act_fn): GELUActivation()\n          (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n        )\n        (decoder): Linear(in_features=128, out_features=20, bias=True)\n      )\n    )\n  )\n)", "parameters": [["encoder.embeddings.word_embeddings.weight", [20, 128]], ["encoder.embeddings.position_embeddings.weight", [100, 128]], ["encoder.embeddings.token_type_embeddings.weight", [2, 128]], ["encoder.embeddings.LayerNorm.weight", [128]], ["encoder.embeddings.LayerNorm.bias", [128]], ["encoder.encoder.layer.0.attention.self.query.weight", [128, 128]], ["encoder.encoder.layer.0.attention.self.query.bias", [128]], ["encoder.encoder.layer.0.attention.self.key.weight", [128, 128]], ["encoder.encoder.layer.0.attention.self.key.bias", [128]], ["encoder.encoder.layer.0.attention.self.value.weight", [128, 128]], ["encoder.encoder.layer.0.attention.self.value.bias", [128]], ["encoder.encoder.layer.0.attention.output.dense.weight", [128, 128]], ["encoder.encoder.layer.0.attention.output.dense.bias", [128]], ["encoder.encoder.layer.0.attention.output.LayerNorm.weight", [128]], ["encoder.encoder.layer.0.attention.output.LayerNorm.bias", [128]], ["encoder.encoder.layer.0.intermediate.dense.weight", [512, 128]], ["encoder.encoder.layer.0.intermediate.dense.bias", [512]], ["encoder.encoder.layer.0.output.dense.weight", [128, 512]], ["encoder.encoder.layer.0.output.dense.bias", [128]], ["encoder.encoder.layer.0.output.LayerNorm.weight", [128]], ["encoder.encoder.layer.0.output.LayerNorm.bias", [128]], ["encoder.encoder.layer.1.attention.self.query.weight", [128, 128]], ["encoder.encoder.layer.1.attention.self.query.bias", [128]], ["encoder.encoder.layer.1.attention.self.key.weight", [128, 128]], ["encoder.encoder.layer.1.attention.self.key.bias", [128]], ["encoder.encoder.layer.1.attention.self.value.weight", [128, 128]], ["encoder.encoder.layer.1.attention.self.value.bias", [128]], ["encoder.encoder.layer.1.attention.output.dense.weight", [128, 128]], ["encoder.encoder.layer.1.attention.output.dense.bias", [128]], ["encoder.encoder.layer.1.attention.output.LayerNorm.weight", [128]], ["encoder.encoder.layer.1.attention.output.LayerNorm.bias", [128]], ["encoder.encoder.layer.1.intermediate.dense.weight", [512, 128]], ["encoder.encoder.layer.1.intermediate.dense.bias", [512]], ["encoder.encoder.layer.1.output.dense.weight", [128, 512]], ["encoder.encoder.layer.1.output.dense.bias", [128]], ["encoder.encoder.layer.1.output.LayerNorm.weight", [128]], ["encoder.encoder.layer.1.output.LayerNorm.bias", [128]], ["encoder.pooler.dense.weight", [128, 128]], ["encoder.pooler.dense.bias", [128]], ["decoder.bert.embeddings.word_embeddings.weight", [20, 128]], ["decoder.bert.embeddings.position_embeddings.weight", [100, 128]], ["decoder.bert.embeddings.token_type_embeddings.weight", [2, 128]], ["decoder.bert.embeddings.LayerNorm.weight", [128]], ["decoder.bert.embeddings.LayerNorm.bias", [128]], ["decoder.bert.encoder.layer.0.attention.self.query.weight", [128, 128]], ["decoder.bert.encoder.layer.0.attention.self.query.bias", [128]], ["decoder.bert.encoder.layer.0.attention.self.key.weight", [128, 128]], ["decoder.bert.encoder.layer.0.attention.self.key.bias", [128]], ["decoder.bert.encoder.layer.0.attention.self.value.weight", [128, 128]], ["decoder.bert.encoder.layer.0.attention.self.value.bias", [128]], ["decoder.bert.encoder.layer.0.attention.output.dense.weight", [128, 128]], ["decoder.bert.encoder.layer.0.attention.output.dense.bias", [128]], ["decoder.bert.encoder.layer.0.attention.output.LayerNorm.weight", [128]], ["decoder.bert.encoder.layer.0.attention.output.LayerNorm.bias", [128]], ["decoder.bert.encoder.layer.0.crossattention.self.query.weight", [128, 128]], ["decoder.bert.encoder.layer.0.crossattention.self.query.bias", [128]], ["decoder.bert.encoder.layer.0.crossattention.self.key.weight", [128, 128]], ["decoder.bert.encoder.layer.0.crossattention.self.key.bias", [128]], ["decoder.bert.encoder.layer.0.crossattention.self.value.weight", [128, 128]], ["decoder.bert.encoder.layer.0.crossattention.self.value.bias", [128]], ["decoder.bert.encoder.layer.0.crossattention.output.dense.weight", [128, 128]], ["decoder.bert.encoder.layer.0.crossattention.output.dense.bias", [128]], ["decoder.bert.encoder.layer.0.crossattention.output.LayerNorm.weight", [128]], ["decoder.bert.encoder.layer.0.crossattention.output.LayerNorm.bias", [128]], ["decoder.bert.encoder.layer.0.intermediate.dense.weight", [512, 128]], ["decoder.bert.encoder.layer.0.intermediate.dense.bias", [512]], ["decoder.bert.encoder.layer.0.output.dense.weight", [128, 512]], ["decoder.bert.encoder.layer.0.output.dense.bias", [128]], ["decoder.bert.encoder.layer.0.output.LayerNorm.weight", [128]], ["decoder.bert.encoder.layer.0.output.LayerNorm.bias", [128]], ["decoder.bert.encoder.layer.1.attention.self.query.weight", [128, 128]], ["decoder.bert.encoder.layer.1.attention.self.query.bias", [128]], ["decoder.bert.encoder.layer.1.attention.self.key.weight", [128, 128]], ["decoder.bert.encoder.layer.1.attention.self.key.bias", [128]], ["decoder.bert.encoder.layer.1.attention.self.value.weight", [128, 128]], ["decoder.bert.encoder.layer.1.attention.self.value.bias", [128]], ["decoder.bert.encoder.layer.1.attention.output.dense.weight", [128, 128]], ["decoder.bert.encoder.layer.1.attention.output.dense.bias", [128]], ["decoder.bert.encoder.layer.1.attention.output.LayerNorm.weight", [128]], ["decoder.bert.encoder.layer.1.attention.output.LayerNorm.bias", [128]], ["decoder.bert.encoder.layer.1.crossattention.self.query.weight", [128, 128]], ["decoder.bert.encoder.layer.1.crossattention.self.query.bias", [128]], ["decoder.bert.encoder.layer.1.crossattention.self.key.weight", [128, 128]], ["decoder.bert.encoder.layer.1.crossattention.self.key.bias", [128]], ["decoder.bert.encoder.layer.1.crossattention.self.value.weight", [128, 128]], ["decoder.bert.encoder.layer.1.crossattention.self.value.bias", [128]], ["decoder.bert.encoder.layer.1.crossattention.output.dense.weight", [128, 128]], ["decoder.bert.encoder.layer.1.crossattention.output.dense.bias", [128]], ["decoder.bert.encoder.layer.1.crossattention.output.LayerNorm.weight", [128]], ["decoder.bert.encoder.layer.1.crossattention.output.LayerNorm.bias", [128]], ["decoder.bert.encoder.layer.1.intermediate.dense.weight", [512, 128]], ["decoder.bert.encoder.layer.1.intermediate.dense.bias", [512]], ["decoder.bert.encoder.layer.1.output.dense.weight", [128, 512]], ["decoder.bert.encoder.layer.1.output.dense.bias", [128]], ["decoder.bert.encoder.layer.1.output.LayerNorm.weight", [128]], ["decoder.bert.encoder.layer.1.output.LayerNorm.bias", [128]], ["decoder.cls.predictions.bias", [20]], ["decoder.cls.predictions.transform.dense.weight", [128, 128]], ["decoder.cls.predictions.transform.dense.bias", [128]], ["decoder.cls.predictions.transform.LayerNorm.weight", [128]], ["decoder.cls.predictions.transform.LayerNorm.bias", [128]]], "output_shape": [[[[0], [0], [0], [0], [0], [0]], [[0], [0], 0, 0, [0], [0], [0], [0], 0, [0], 0, 0, [0], 0, 0], [[0], 0, [0], 0, 0, 0, [0], 0, [0], 0, 0, 0, 0, [0], 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]], "num_parameters": [2560, 12800, 256, 128, 128, 16384, 128, 16384, 128, 16384, 128, 16384, 128, 128, 128, 65536, 512, 65536, 128, 128, 128, 16384, 128, 16384, 128, 16384, 128, 16384, 128, 128, 128, 65536, 512, 65536, 128, 128, 128, 16384, 128, 2560, 12800, 256, 128, 128, 16384, 128, 16384, 128, 16384, 128, 16384, 128, 128, 128, 16384, 128, 16384, 128, 16384, 128, 16384, 128, 128, 128, 65536, 512, 65536, 128, 128, 128, 16384, 128, 16384, 128, 16384, 128, 16384, 128, 128, 128, 16384, 128, 16384, 128, 16384, 128, 16384, 128, 128, 128, 65536, 512, 65536, 128, 128, 128, 20, 16384, 128, 128, 128]}, {"name": "model_z_to_x", "id": 10829094288, "class_name": "EncoderDecoderModel(\n  (encoder): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(20, 128, padding_idx=0)\n      (position_embeddings): Embedding(100, 128)\n      (token_type_embeddings): Embedding(2, 128)\n      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-1): 2 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=128, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=128, out_features=128, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (decoder): BertLMHeadModel(\n    (bert): BertModel(\n      (embeddings): BertEmbeddings(\n        (word_embeddings): Embedding(20, 128, padding_idx=0)\n        (position_embeddings): Embedding(100, 128)\n        (token_type_embeddings): Embedding(2, 128)\n        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (encoder): BertEncoder(\n        (layer): ModuleList(\n          (0-1): 2 x BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=128, out_features=128, bias=True)\n                (key): Linear(in_features=128, out_features=128, bias=True)\n                (value): Linear(in_features=128, out_features=128, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=128, out_features=128, bias=True)\n                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (crossattention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=128, out_features=128, bias=True)\n                (key): Linear(in_features=128, out_features=128, bias=True)\n                (value): Linear(in_features=128, out_features=128, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=128, out_features=128, bias=True)\n                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=128, out_features=512, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=512, out_features=128, bias=True)\n              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n    )\n    (cls): BertOnlyMLMHead(\n      (predictions): BertLMPredictionHead(\n        (transform): BertPredictionHeadTransform(\n          (dense): Linear(in_features=128, out_features=128, bias=True)\n          (transform_act_fn): GELUActivation()\n          (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n        )\n        (decoder): Linear(in_features=128, out_features=20, bias=True)\n      )\n    )\n  )\n)", "parameters": [["encoder.embeddings.word_embeddings.weight", [20, 128]], ["encoder.embeddings.position_embeddings.weight", [100, 128]], ["encoder.embeddings.token_type_embeddings.weight", [2, 128]], ["encoder.embeddings.LayerNorm.weight", [128]], ["encoder.embeddings.LayerNorm.bias", [128]], ["encoder.encoder.layer.0.attention.self.query.weight", [128, 128]], ["encoder.encoder.layer.0.attention.self.query.bias", [128]], ["encoder.encoder.layer.0.attention.self.key.weight", [128, 128]], ["encoder.encoder.layer.0.attention.self.key.bias", [128]], ["encoder.encoder.layer.0.attention.self.value.weight", [128, 128]], ["encoder.encoder.layer.0.attention.self.value.bias", [128]], ["encoder.encoder.layer.0.attention.output.dense.weight", [128, 128]], ["encoder.encoder.layer.0.attention.output.dense.bias", [128]], ["encoder.encoder.layer.0.attention.output.LayerNorm.weight", [128]], ["encoder.encoder.layer.0.attention.output.LayerNorm.bias", [128]], ["encoder.encoder.layer.0.intermediate.dense.weight", [512, 128]], ["encoder.encoder.layer.0.intermediate.dense.bias", [512]], ["encoder.encoder.layer.0.output.dense.weight", [128, 512]], ["encoder.encoder.layer.0.output.dense.bias", [128]], ["encoder.encoder.layer.0.output.LayerNorm.weight", [128]], ["encoder.encoder.layer.0.output.LayerNorm.bias", [128]], ["encoder.encoder.layer.1.attention.self.query.weight", [128, 128]], ["encoder.encoder.layer.1.attention.self.query.bias", [128]], ["encoder.encoder.layer.1.attention.self.key.weight", [128, 128]], ["encoder.encoder.layer.1.attention.self.key.bias", [128]], ["encoder.encoder.layer.1.attention.self.value.weight", [128, 128]], ["encoder.encoder.layer.1.attention.self.value.bias", [128]], ["encoder.encoder.layer.1.attention.output.dense.weight", [128, 128]], ["encoder.encoder.layer.1.attention.output.dense.bias", [128]], ["encoder.encoder.layer.1.attention.output.LayerNorm.weight", [128]], ["encoder.encoder.layer.1.attention.output.LayerNorm.bias", [128]], ["encoder.encoder.layer.1.intermediate.dense.weight", [512, 128]], ["encoder.encoder.layer.1.intermediate.dense.bias", [512]], ["encoder.encoder.layer.1.output.dense.weight", [128, 512]], ["encoder.encoder.layer.1.output.dense.bias", [128]], ["encoder.encoder.layer.1.output.LayerNorm.weight", [128]], ["encoder.encoder.layer.1.output.LayerNorm.bias", [128]], ["encoder.pooler.dense.weight", [128, 128]], ["encoder.pooler.dense.bias", [128]], ["decoder.bert.embeddings.word_embeddings.weight", [20, 128]], ["decoder.bert.embeddings.position_embeddings.weight", [100, 128]], ["decoder.bert.embeddings.token_type_embeddings.weight", [2, 128]], ["decoder.bert.embeddings.LayerNorm.weight", [128]], ["decoder.bert.embeddings.LayerNorm.bias", [128]], ["decoder.bert.encoder.layer.0.attention.self.query.weight", [128, 128]], ["decoder.bert.encoder.layer.0.attention.self.query.bias", [128]], ["decoder.bert.encoder.layer.0.attention.self.key.weight", [128, 128]], ["decoder.bert.encoder.layer.0.attention.self.key.bias", [128]], ["decoder.bert.encoder.layer.0.attention.self.value.weight", [128, 128]], ["decoder.bert.encoder.layer.0.attention.self.value.bias", [128]], ["decoder.bert.encoder.layer.0.attention.output.dense.weight", [128, 128]], ["decoder.bert.encoder.layer.0.attention.output.dense.bias", [128]], ["decoder.bert.encoder.layer.0.attention.output.LayerNorm.weight", [128]], ["decoder.bert.encoder.layer.0.attention.output.LayerNorm.bias", [128]], ["decoder.bert.encoder.layer.0.crossattention.self.query.weight", [128, 128]], ["decoder.bert.encoder.layer.0.crossattention.self.query.bias", [128]], ["decoder.bert.encoder.layer.0.crossattention.self.key.weight", [128, 128]], ["decoder.bert.encoder.layer.0.crossattention.self.key.bias", [128]], ["decoder.bert.encoder.layer.0.crossattention.self.value.weight", [128, 128]], ["decoder.bert.encoder.layer.0.crossattention.self.value.bias", [128]], ["decoder.bert.encoder.layer.0.crossattention.output.dense.weight", [128, 128]], ["decoder.bert.encoder.layer.0.crossattention.output.dense.bias", [128]], ["decoder.bert.encoder.layer.0.crossattention.output.LayerNorm.weight", [128]], ["decoder.bert.encoder.layer.0.crossattention.output.LayerNorm.bias", [128]], ["decoder.bert.encoder.layer.0.intermediate.dense.weight", [512, 128]], ["decoder.bert.encoder.layer.0.intermediate.dense.bias", [512]], ["decoder.bert.encoder.layer.0.output.dense.weight", [128, 512]], ["decoder.bert.encoder.layer.0.output.dense.bias", [128]], ["decoder.bert.encoder.layer.0.output.LayerNorm.weight", [128]], ["decoder.bert.encoder.layer.0.output.LayerNorm.bias", [128]], ["decoder.bert.encoder.layer.1.attention.self.query.weight", [128, 128]], ["decoder.bert.encoder.layer.1.attention.self.query.bias", [128]], ["decoder.bert.encoder.layer.1.attention.self.key.weight", [128, 128]], ["decoder.bert.encoder.layer.1.attention.self.key.bias", [128]], ["decoder.bert.encoder.layer.1.attention.self.value.weight", [128, 128]], ["decoder.bert.encoder.layer.1.attention.self.value.bias", [128]], ["decoder.bert.encoder.layer.1.attention.output.dense.weight", [128, 128]], ["decoder.bert.encoder.layer.1.attention.output.dense.bias", [128]], ["decoder.bert.encoder.layer.1.attention.output.LayerNorm.weight", [128]], ["decoder.bert.encoder.layer.1.attention.output.LayerNorm.bias", [128]], ["decoder.bert.encoder.layer.1.crossattention.self.query.weight", [128, 128]], ["decoder.bert.encoder.layer.1.crossattention.self.query.bias", [128]], ["decoder.bert.encoder.layer.1.crossattention.self.key.weight", [128, 128]], ["decoder.bert.encoder.layer.1.crossattention.self.key.bias", [128]], ["decoder.bert.encoder.layer.1.crossattention.self.value.weight", [128, 128]], ["decoder.bert.encoder.layer.1.crossattention.self.value.bias", [128]], ["decoder.bert.encoder.layer.1.crossattention.output.dense.weight", [128, 128]], ["decoder.bert.encoder.layer.1.crossattention.output.dense.bias", [128]], ["decoder.bert.encoder.layer.1.crossattention.output.LayerNorm.weight", [128]], ["decoder.bert.encoder.layer.1.crossattention.output.LayerNorm.bias", [128]], ["decoder.bert.encoder.layer.1.intermediate.dense.weight", [512, 128]], ["decoder.bert.encoder.layer.1.intermediate.dense.bias", [512]], ["decoder.bert.encoder.layer.1.output.dense.weight", [128, 512]], ["decoder.bert.encoder.layer.1.output.dense.bias", [128]], ["decoder.bert.encoder.layer.1.output.LayerNorm.weight", [128]], ["decoder.bert.encoder.layer.1.output.LayerNorm.bias", [128]], ["decoder.cls.predictions.bias", [20]], ["decoder.cls.predictions.transform.dense.weight", [128, 128]], ["decoder.cls.predictions.transform.dense.bias", [128]], ["decoder.cls.predictions.transform.LayerNorm.weight", [128]], ["decoder.cls.predictions.transform.LayerNorm.bias", [128]]], "output_shape": [[[[0], [0], [0], [0], [0], [0]], [[0], [0], 0, 0, [0], [0], [0], [0], 0, [0], 0, 0, [0], 0, 0], [[0], 0, [0], 0, 0, 0, [0], 0, [0], 0, 0, 0, 0, [0], 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]], "num_parameters": [2560, 12800, 256, 128, 128, 16384, 128, 16384, 128, 16384, 128, 16384, 128, 128, 128, 65536, 512, 65536, 128, 128, 128, 16384, 128, 16384, 128, 16384, 128, 16384, 128, 128, 128, 65536, 512, 65536, 128, 128, 128, 16384, 128, 2560, 12800, 256, 128, 128, 16384, 128, 16384, 128, 16384, 128, 16384, 128, 128, 128, 16384, 128, 16384, 128, 16384, 128, 16384, 128, 128, 128, 65536, 512, 65536, 128, 128, 128, 16384, 128, 16384, 128, 16384, 128, 16384, 128, 128, 128, 16384, 128, 16384, 128, 16384, 128, 16384, 128, 128, 128, 65536, 512, 65536, 128, 128, 128, 20, 16384, 128, 128, 128]}, {"name": "completeness_x", "id": 10829450640, "class_name": "Completeness()", "parameters": [], "output_shape": [], "num_parameters": []}, {"name": "completeness_z", "id": 10809148112, "class_name": "Completeness()", "parameters": [], "output_shape": [], "num_parameters": []}, {"name": "homogeneity_x", "id": 10829283152, "class_name": "SentenceHomogeneity()", "parameters": [], "output_shape": [], "num_parameters": []}, {"name": "homogeneity_z", "id": 10829096528, "class_name": "SentenceHomogeneity()", "parameters": [], "output_shape": [], "num_parameters": []}, {"name": "accuracy_x", "id": 10829451728, "class_name": "Accuracy()", "parameters": [], "output_shape": [[]], "num_parameters": []}, {"name": "accuracy_z", "id": 10829281872, "class_name": "Accuracy()", "parameters": [], "output_shape": [[]], "num_parameters": []}, {"name": "accuracy_x_sentence", "id": 10829453200, "class_name": "Accuracy()", "parameters": [], "output_shape": [[]], "num_parameters": []}, {"name": "accuracy_z_sentence", "id": 10825369104, "class_name": "Accuracy()", "parameters": [], "output_shape": [[]], "num_parameters": []}, {"name": "token_homogeneity_x", "id": 10829449616, "class_name": "TokenHomogeneity()", "parameters": [], "output_shape": [], "num_parameters": []}, {"name": "token_homogeneity_z", "id": 10829089104, "class_name": "TokenHomogeneity()", "parameters": [], "output_shape": [], "num_parameters": []}], "edges": []}